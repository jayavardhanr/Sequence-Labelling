{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports come here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda/envs/slp_presentation/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.5\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import codecs\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional,Input,merge #ChainCRF\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.utils.np_utils import accuracy\n",
    "#from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D #Convolution2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = OrderedDict()\n",
    "parameters['train'] = \"./data/eng.train\" #Path to train file\n",
    "parameters['dev'] = \"./data/eng.testa\" #Path to test file\n",
    "parameters['test'] = \"./data/eng.testb\" #Path to dev file\n",
    "parameters['tag_scheme'] = \"iob\" #iob or iobes\n",
    "parameters['lower'] = True # Boolean variable to control lowercasing of words\n",
    "parameters['zeros'] =  True # Boolean variable to control replacement of  all digits by 0 \n",
    "parameters['char_dim'] = 30 #Char embedding dimension\n",
    "parameters['char_lstm_dim'] = 25 #Char LSTM hidden layer size\n",
    "parameters['char_bidirect'] = True #Use a bidirectional LSTM for chars\n",
    "parameters['word_dim'] = 100 #Token embedding dimension\n",
    "parameters['word_lstm_dim'] = 200 #Token LSTM hidden layer size\n",
    "parameters['word_bidirect'] = True #Use a bidirectional LSTM for words\n",
    "parameters['embedding_path'] = \"./embeddings/glove/glove.6B.100d.txt\" #Location of pretrained embeddings\n",
    "#parameters['all_emb'] = 1 #Load all embeddings\n",
    "parameters['cap_dim'] = 4 #Capitalization feature dimension (0 to disable)\n",
    "parameters['crf'] =1 #Use CRF (0 to disable)\n",
    "parameters['dropout'] = 0.5 #Droupout on the input (0 = no dropout)\n",
    "#parameters['lr_method'] = \"sgd-lr_.005\" #Learning method (SGD, Adadelta, Adam..)\n",
    "parameters['epoch'] =  50 #Number of epochs to run\"\n",
    "parameters['weights'] = \"\" #path to Pretrained for from a previous run\n",
    "parameters['reload'] = \"\" #Path to Reload the last saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences(path, lower, zeros):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_sentences(parameters['train'], parameters['lower'], parameters['zeros'])\n",
    "test_sentences = load_sentences(parameters['test'], parameters['lower'], parameters['zeros'])\n",
    "dev_sentences = load_sentences(parameters['dev'], parameters['lower'], parameters['zeros'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag format checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to modify and check Tagging Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iob2(tags):\n",
    "    \"\"\"\n",
    "    Check that tags have a valid IOB format.\n",
    "    Tags in IOB1 format are converted to IOB2.\n",
    "    \"\"\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            continue\n",
    "        split = tag.split('-')\n",
    "        if len(split) != 2 or split[0] not in ['I', 'B']:\n",
    "            return False\n",
    "        if split[0] == 'B':\n",
    "            continue\n",
    "        elif i == 0 or tags[i - 1] == 'O':  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "        elif tags[i - 1][1:] == tag[1:]:\n",
    "            continue\n",
    "        else:  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "    return True\n",
    "\n",
    "\n",
    "def iob_iobes(tags):\n",
    "    \"\"\"\n",
    "    IOB -> IOBES\n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'B':\n",
    "            if i + 1 != len(tags) and \\\n",
    "               tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('B-', 'S-'))\n",
    "        elif tag.split('-')[0] == 'I':\n",
    "            if i + 1 < len(tags) and \\\n",
    "                    tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('I-', 'E-'))\n",
    "        else:\n",
    "            raise Exception('Invalid IOB format!')\n",
    "    return new_tags\n",
    "\n",
    "\n",
    "def iobes_iob(tags):\n",
    "    \"\"\"\n",
    "    IOBES -> IOB\n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag.split('-')[0] == 'B':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'I':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'S':\n",
    "            new_tags.append(tag.replace('S-', 'B-'))\n",
    "        elif tag.split('-')[0] == 'E':\n",
    "            new_tags.append(tag.replace('E-', 'I-'))\n",
    "        elif tag.split('-')[0] == 'O':\n",
    "            new_tags.append(tag)\n",
    "        else:\n",
    "            raise Exception('Invalid format!')\n",
    "    return new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tag_scheme(sentences, tag_scheme):\n",
    "    \"\"\"\n",
    "    Check and update sentences tagging scheme to IOB2.\n",
    "    Only IOB1 and IOB2 schemes are accepted.\n",
    "    \"\"\"\n",
    "    for i, s in enumerate(sentences):\n",
    "        tags = [w[-1] for w in s]\n",
    "        # Check that tags are given in the IOB format\n",
    "        if not iob2(tags):\n",
    "            s_str = '\\n'.join(' '.join(w) for w in s)\n",
    "            raise Exception('Sentences should be given in IOB format! ' +\n",
    "                            'Please check sentence %i:\\n%s' % (i, s_str))\n",
    "        if tag_scheme == 'iob':\n",
    "            # If format was IOB1, we convert to IOB2\n",
    "            for word, new_tag in zip(s, tags):\n",
    "                word[-1] = new_tag\n",
    "        elif tag_scheme == 'iobes':\n",
    "            new_tags = iob_iobes(tags) #convert data in iob to iobes format\n",
    "            for word, new_tag in zip(s, new_tags):\n",
    "                word[-1] = new_tag\n",
    "        else:\n",
    "            raise Exception('Unknown tagging scheme!')\n",
    "\n",
    "update_tag_scheme(train_sentences, parameters['tag_scheme'])\n",
    "update_tag_scheme(dev_sentences, parameters['tag_scheme'])\n",
    "update_tag_scheme(test_sentences, parameters['tag_scheme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences, lower):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    words = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<UNK>'] = 10000000\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\"Found %i unique words (%i in total)\" % (\n",
    "        len(dico), sum(len(x) for x in words)\n",
    "    ))\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "def char_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and mapping of characters, sorted by frequency.\n",
    "    \"\"\"\n",
    "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
    "    dico = create_dico(chars)\n",
    "    char_to_id, id_to_char = create_mapping(dico)\n",
    "    print(\"Found %i unique characters\" % len(dico))\n",
    "    return dico, char_to_id, id_to_char\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17493 unique words (203621 in total)\n",
      "Found 75 unique characters\n",
      "Found 9 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "dico_words,word_to_id,id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
    "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_feature(s):\n",
    "    \"\"\"\n",
    "    Capitalization feature:\n",
    "    0 = low caps\n",
    "    1 = all caps\n",
    "    2 = first letter caps\n",
    "    3 = one capital (not first letter)\n",
    "    \"\"\"\n",
    "    if s.lower() == s:\n",
    "        return 0\n",
    "    elif s.upper() == s:\n",
    "        return 1\n",
    "    elif s[0].upper() == s[0]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041 / 3250 / 3453 sentences in train / dev / test.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(sentences, word_to_id, char_to_id, tag_to_id, lower=False):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
    "        - word indexes\n",
    "        - word char indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    def f(x): return x.lower() if lower else x\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = [word_to_id[f(w) if f(w) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        # Skip characters that are not in the training set\n",
    "        chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
    "                 for w in str_words]\n",
    "        caps = [cap_feature(w) for w in str_words]\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words': words,\n",
    "            'chars': chars,\n",
    "            'caps': caps,\n",
    "            'tags': tags,\n",
    "        })\n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "print(\"{} / {} / {} sentences in train / dev / test.\".format(len(train_data), len(dev_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Parameters\n",
    "parameters['word_vocab_size'] = len(dico_words.keys()) #total vocab size for words\n",
    "parameters['char_vocab_size'] = len(dico_chars.keys()) #total vocab size for characters\n",
    "parameters['batch_size'] = 10 #Batch Size\n",
    "parameters['cnn_nb_filters']  = 30 #number of filters in CNN\n",
    "parameters['cnn_window_length']= 3 #window length in CNN\n",
    "\n",
    "#SGD parameters\n",
    "parameters['learning_rate'] = 0.015 #learning rate for SGD\n",
    "parameters['decay_rate'] = 0.05 #decay rate for the learning rate\n",
    "parameters['momentum'] = 0.9 #momentum paramter for the SGD\n",
    "parameters['clipvalue'] = 5.0 #gradient clipping value\n",
    "parameters['max_words'] = 100 #a sentence can have atmost 100 words\n",
    "parameters['max_chars'] = 20 #a word can ahve atmost 20 char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_parameter (dataSet, key,max_words,max_chars):\n",
    "    str_words=[]\n",
    "    for row in dataSet:\n",
    "        words = row[key]\n",
    "        nb_remaining = max_words - len(words)\n",
    "        sent  = []\n",
    "        if(nb_remaining > 0 ):\n",
    "            for i in range (nb_remaining):\n",
    "                sent.append(\"<UNK>\")\n",
    "                \n",
    "        #we clip sentences bigger than 100 words\n",
    "        word_length = min(len(words) , max_words)\n",
    "        sent = sent+words[0:word_length]\n",
    "        str_words.append(sent)\n",
    "    return str_words\n",
    "\n",
    "def CreateX_Y(dataSet,max_words=100,maxCharLength=20):\n",
    "    Words_id = []\n",
    "    tag = []\n",
    "    caps =[]\n",
    "    char = []\n",
    "    str_words = []\n",
    "    for row in dataSet:\n",
    "        Words_id.append(row['words'])\n",
    "        \n",
    "    str_words = pad_parameter(dataSet,'str_words',max_words,maxCharLength)\n",
    "    \n",
    "    #Dont have the call to the function above because \n",
    "    #we have padsequences function below\n",
    "    for row in dataSet:\n",
    "        tag.append(row['tags'])\n",
    "\n",
    "    for row in dataSet:\n",
    "        caps.append(row['caps'])\n",
    "      \n",
    "    for row in dataSet:\n",
    "        char_1 = row['chars']\n",
    "        nb_remaining = max_words - len(char_1)\n",
    "        sentence = []\n",
    "        if(nb_remaining > 0 ):\n",
    "            sentence = [0] * maxCharLength * nb_remaining\n",
    "        wordList = []\n",
    "        wordCount = 0;\n",
    "        for word in char_1[0:max_words]:\n",
    "            padding = [0] * (maxCharLength - len(word))\n",
    "            word_pad = padding +word[0:maxCharLength]\n",
    "            #print (word_pad)\n",
    "            sentence = sentence + word_pad   \n",
    "        #we want 100 words per sentence, each of which has 20 char\n",
    "        char.append(sentence)\n",
    "        \n",
    "    Words_id= pad_sequences(Words_id,maxlen=max_words)\n",
    "    \n",
    "    tag = pad_sequences(tag,maxlen=max_words)   \n",
    "    caps = pad_sequences(caps,maxlen=max_words)\n",
    "    char = np.asarray(char)\n",
    "    \n",
    "    #char = char.reshape(char.shape[0],max_words*maxCharLength)\n",
    "    return Words_id,tag,caps,char,str_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words_id_train,tag_train,caps_train,char_train,Words_str_train = \\\n",
    "                                CreateX_Y(train_data,parameters['max_words'],parameters['max_chars'])\n",
    "Words_id_test,tag_test,caps_test,char_test,Words_str_test= \\\n",
    "                                CreateX_Y(test_data,parameters['max_words'],parameters['max_chars'])\n",
    "Words_id_dev,tag_dev,caps_dev,char_dev,Words_str_dev = \\\n",
    "                                CreateX_Y(dev_data,parameters['max_words'],parameters['max_chars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['tag_label_size'] = len(tag_to_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_train = np.expand_dims(tag_train, -1)\n",
    "tag_dev = np.expand_dims(tag_dev, -1)\n",
    "tag_test = np.expand_dims(tag_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embed_matrix(word_to_id,ext_emb_path,word_vocab_size,word_embedding_dim):\n",
    "    #based on https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "    embeddings_index = {}\n",
    "    f = open(ext_emb_path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    embedding_weights_range = math.sqrt(3/word_embedding_dim)\n",
    "    embedding_matrix = np.zeros((word_vocab_size + 1, word_embedding_dim))\n",
    "    for word, i in word_to_id.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = np.random.uniform(low=-embedding_weights_range, high= embedding_weights_range,size = (1,100))\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = initialize_embed_matrix(word_to_id,parameters['embedding_path'],parameters['word_vocab_size'],parameters['word_dim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(parameters,embedding_matrix =None, weightsPath = None):\n",
    "    lstm_dim = parameters['word_lstm_dim']\n",
    "    word_vocab_size = parameters['word_vocab_size'] \n",
    "    char_vocab_size = parameters['char_vocab_size']\n",
    "    char_embedding_dim = parameters['char_dim']\n",
    "    word_embedding_dim = parameters['word_dim']\n",
    "    maxCharSize = parameters['max_chars']\n",
    "    cap_size = \tparameters['cap_size']\n",
    "    cap_embed_size = parameters['cap_dim']\n",
    "    max_words = parameters['max_words']\n",
    "    nb_filters = parameters['cnn_nb_filters']\n",
    "    window_length = parameters['cnn_window_length']\n",
    "    learning_rate = parameters['learning_rate']\n",
    "    decay_rate = parameters['decay_rate'] \n",
    "    momentum = parameters['momentum']\n",
    "    clipvalue = parameters['clipvalue']\n",
    "    tag_label_size = parameters['tag_label_size']\n",
    "    dropout = parameters['dropout']\n",
    "\n",
    "    char_input = Input(shape=(maxCharSize * max_words,), dtype='int32', name='char_input')\n",
    "    char_emb = Embedding(char_vocab_size, char_embedding_dim, input_length=max_words*maxCharSize, dropout=dropout, name='char_emb')(char_input)\n",
    "    char_cnn = Convolution1D(nb_filter=nb_filters,filter_length= window_length, activation='tanh', border_mode='full') (char_emb) \n",
    "    char_max_pooling = MaxPooling1D(pool_length=maxCharSize) (char_cnn) #  get output per word. this is the size of the hidden layer\n",
    "\n",
    "    #based on https://github.com/pressrelations/keras/blob/a2d358e17ea7979983c3c6704390fe2d4b29bbbf/examples/conll2000_bi_lstm_crf.py\n",
    "    word_input = Input(shape=(max_words,), dtype='int32', name='word_input')\n",
    "    if (embedding_matrix is not None):\n",
    "        word_emb = Embedding(word_vocab_size+1, word_embedding_dim,weights=[embedding_matrix], input_length=max_words, dropout=0, name='word_emb')(word_input)\n",
    "    else:\n",
    "        word_emb = Embedding(word_vocab_size+1, word_embedding_dim, input_length=max_words, dropout=0, name='word_emb')(word_input)\n",
    "\n",
    "    caps_input = Input(shape=(max_words,), dtype='int32', name='caps_input')\n",
    "    caps_emb = Embedding(cap_size, cap_embed_size, input_length=None, dropout=dropout, name='caps_emb')(caps_input)\n",
    "    #concat axis refers to the axis whose dimension can be different\n",
    "    total_emb = merge([word_emb, caps_emb,char_max_pooling], mode='concat', concat_axis=2,name ='total_emb')\n",
    "    emb_droput = Dropout(dropout)(total_emb)\n",
    "    #inner_init : initialization function of the inner cells. I believe this is Cell state\n",
    "    bilstm_word  = Bidirectional(LSTM(lstm_dim,inner_init='uniform', forget_bias_init='one',return_sequences=True))(emb_droput)\n",
    "    bilstm_word_d = Dropout(dropout)(bilstm_word)\n",
    "\n",
    "    dense = TimeDistributed(Dense(tag_label_size))(bilstm_word_d)\n",
    "    crf = ChainCRF()def shared(shape, name):\n",
    "    crf_output = crf(dense)\n",
    "    #to accoutn for gradient clipping\n",
    "    #info on nesterov http://stats.stackexchange.com/questions/211334/keras-how-does-sgd-learning-rate-decay-work\n",
    "    sgd = SGD(lr=learning_rate, decay=decay_rate, momentum=momentum, nesterov=False,clipvalue = clipvalue)\n",
    "\n",
    "\n",
    "\n",
    "    model = Model(input=[word_input,caps_input,char_input], output=[crf_output])\n",
    "    if(weightsPath):\n",
    "        model.load_weights(weightsPath)\n",
    "    model.compile(loss=crf.sparse_loss,\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def train_model (model,parameters,Words_id_train,caps_train,char_train,tag_train,Words_id_dev=None,caps_dev=None,char_dev = None,tag_dev=None):\n",
    "\n",
    "    # define the checkpoint\n",
    "    filepath=\"weights-improvement-BiLSTM-All-no-wd-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    batch_size = parameters['batch_size']\n",
    "    epoch_number = parameters['epoch']\n",
    "    model.fit([Words_id_train,caps_train,char_train], tag_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=([Words_id_dev,caps_dev,char_dev], tag_dev), nb_epoch=epoch_number,callbacks=callbacks_list)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if parameters['pre_emb'] and weights:\n",
    "    assert os.path.isfile(weights)\n",
    "    model = build_model(parameters,embedding_matrix=embedding_matrix,weightsPath=weights)\n",
    "elif parameters['pre_emb']:\n",
    "    model = build_model(parameters,embedding_matrix=embedding_matrix)\n",
    "elif weights:\n",
    "    assert os.path.isfile(weights)\n",
    "    model = build_model(parameters,weightsPath =weights)\n",
    "else:\n",
    "    model = build_model(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_to_id, open(\"word_to_id.pkl\",'wb'))\n",
    "pickle.dump(char_to_id, open(\"char_to_id.pkl\",'wb'))\n",
    "pickle.dump(tag_to_id, open(\"tag_to_id.pkl\",'wb'))\n",
    "pickle.dump(id_to_tag,open(\"id_to_tag.pkl\",'wb'))\n",
    "pickle.dump(parameters,open(\"parameters.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model (model,parameters,Words_id_train,caps_train,char_train,tag_train,Words_id_dev,caps_dev,char_dev,tag_dev)\n",
    "scores = model.evaluate([Words_id_test, caps_test,char_test], tag_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
